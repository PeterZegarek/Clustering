Question 1:
    1a. The k-means algorithm makes the assumptions that all the data is a double, each line of data is of the same dimension, there is a
    predifined number of clusters beforehand, and Euclidean distance can be calculated from the data. In terms of more general k-means
    assumptions, it assumes that clusters are spherical in shape, and the 'distance' of the cluster is about the same in every direction.
    Also, for every cluster, the distribution of points around the center should be about the same, and the clusters should be roughly
    the same size.
    1b. 
        Always Succeed - bullseye2, diffdensity2, diffstddev2, easygaussian1, easygaussian2, easygaussian3, easygaussian5, hardgaussian1,
        hardgaussian2, hardgaussian3, hardgaussian5, hardgaussian8
        Sometimes Succeed - easygaussian4, easygaussian6, easygaussian7, easygaussian9, hardgaussian4, hardgaussian6, hardgaussian7
        Never Succeed - easygaussian8
    1c. The ones that sometimes succeed and never succeed tend to have a higher number of clusters. Because they all have 100 data points,
    more clusters would mean that the size of clusters are fairly small. When this happens, the randomly picked centroids are more important
    to determining where the clusters are formed. So in these cases, some assumptions can be violated:
        i. Clusters are spherical. With many clusters, it may be possible that the ideal clusters don't all have a spherical shape.
        ii. The 'distance' of the cluster is about the same in every direction. Same as the one before, some clusters may not have that
        uniform shape if there are too many of them.
        iii. Clusters should be roughly the same size. Depending on the spread of data, some clusters should be significantly larger
        or smaller than others for it to make sense. 
    1d. Yes, K-means clustering can fail even if assumptions are not violated. The forgy initialization of the algorithm leads to random
    data points being picked for the initial centroids. In some cases, this can skew the clusters of data, leading to unideal clusters.
    The data can also have outliers. This can significantly affect the centroids, leading to more clusters that aren't ideal.

Question 2:
    2a. The iterated use with its randomness allows us to see more varied and possible better results with each iteration. The more iterations there are, the more of a chance of better results due to the randomness of the starting point.
    Since the clusters start from random points, they can converge in different places (like local maximums, conceptually). By running this multiple times and taking the lowest WCSS, we can output the highest quality clusters.
    2b. The iterated sample run appears to succeed for bullseye2, diffdensity2, diffstddev2, easygaussian1, easygaussian2, easygaussian3, easygaussian4, hardgaussian1, hardgaussian2, hardgaussian3, hardgaussian4, hardgaussian5, and stretched2.
    2c. This iterated approach helps different densities, different standard deviations, stretched problems, and sometimes gaussian problems.

Question 3:
    3a. For each input file provided, run your program 8 times with kMin=2 and kMax=10 and iter=10. From each run, record the number of clusters k that yielded the maximum gap statistic in Columns J-Q of the spreadsheet.
    3b. For which data set(s) does this technique consistently succeed in discerning the correct number of clusters?
    3c. For which data set(s) does the discerned number of clusters vary from run to run?
    3d. For which data set(s) does this technique consistently return the incorrect number of clusters? What do you observe about the nature of the data in these case(s)?
